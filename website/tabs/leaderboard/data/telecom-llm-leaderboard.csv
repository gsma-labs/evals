"Rank","Provider","Model","Repo","Mean","TeleQna","TeleLogs","TeleMath","3GPP-TSG","TeleYAML"
"1","OpenAI","GPT-5","openai/gpt-5","65.55","82.51 (raw)","80.00 (raw)","70.27 (raw)","67.90 (raw)","27.07 (llm-as-judge)"
"2","Grok","Grok-4-fast","grok/grok-4-fast","61.52","79.39 (raw)","78.12 (raw)","62.80 (raw)","60.60 (raw)","26.67 (llm-as-judge)"
"3","Claude","Claude-Sonnet-4.5","claude/claude-sonnet-4.5","60.64","80.57 (raw)","67.12 (raw)","62.80 (raw)","65.25 (raw)","27.47 (llm-as-judge)"
"4","Google","Gemini-2.5-pro","google/gemini-2.5-pro","58.44","80.11 (raw)","44.30 (raw)","74.40 (raw)","68.20 (raw)","25.20 (llm-as-judge)"
"5","NetoAI","TSLAM-18B","NetoAI/TSLAM-18B","49.93","72.00 (raw)","20.62 (raw)","69.50 (raw)","63.50 (raw)","24.05 (llm-as-judge)"
"6","OpenAI","GPT-OSS-120B","openai/gpt-oss-120b","49.71","78.51 (raw)","44.70 (raw)","60.40 (raw)","36.25 (raw)","28.70 (llm-as-judge)"
"7","Qwen","Qwen3-32B","qwen/qwen3-32b","47.46","76.14 (raw)","33.77 (raw)","69.51 (raw)","34.78 (raw)","23.10 (llm-as-judge)"
"8","Mistral","Mistral-Large-123B","mistralai/Mistral-Large-Instruct-2411","44.93","75.85 (raw)","30.55 (raw)","38.80 (raw)","54.95 (raw)","24.50 (llm-as-judge)"
"9","OpenAI","GPT-OSS-20B","openai/gpt-oss-20b","43.97","75.79 (raw)","40.10 (raw)","53.80 (raw)","30.18 (raw)","20.00 (llm-as-judge)"
"10","Meta","Llama-3.3-70B-Instruct","meta/llama-3.3-70b-instruct","42.40","74.98 (raw)","22.01 (raw)","36.23 (raw)","55.18 (raw)","23.60 (llm-as-judge)"
"11","Qwen","Qwen3-8B","qwen/qwen3-8b","42.18","73.21 (raw)","36.42 (raw)","49.73 (raw)","31.42 (raw)","20.13 (llm-as-judge)"
"12","Qwen","Qwen3-4B","qwen/qwen3-4b","40.57","70.50 (raw)","32.00 (raw)","45.62 (raw)","31.65 (raw)","23.10 (llm-as-judge)"
"13","NetoAI","TSLAM-G3","NetoAI/TSLAM-G3","40.10","82.50 (raw)","11.25 (raw)","26.50 (raw)","58.50 (raw)","21.73 (llm-as-judge)"
"14","IBM Granite","granite-4.0-h-small","ibm-granite/granite-4.0-h-small","35.16","72.15 (raw)","17.24 (raw)","32.40 (raw)","32.53 (raw)","21.50 (llm-as-judge)"
"15","DeepSeek","DeepSeek-R1-Distill-Qwen-7B","deepseek-ai/DeepSeek-R1-Distill-Qwen-7B","29.48","69.31 (raw)","12.05 (raw)","24.90 (raw)","21.43 (raw)","19.73 (llm-as-judge)"
"16","Meta","Llama-3.1-8B-Instruct","meta/llama-3.1-8b-instruct","28.11","68.03 (raw)","13.42 (raw)","13.56 (raw)","25.27 (raw)","20.27 (llm-as-judge)"
"17","IBM Granite","granite-3.3-8b-instruct","ibm-granite/granite-3.3-8b-instruct","27.28","62.35 (raw)","13.31 (raw)","14.37 (raw)","26.00 (raw)","20.39 (llm-as-judge)"
"18","LiquidAI","LFM2-2.6B","liquidai/LFM2-2.6B","25.79","57.90 (raw)","9.08 (raw)","18.05 (raw)","24.20 (raw)","19.73 (llm-as-judge)"
"19","NetoAI","TSLAM-2B MINI","NetoAI/TSLAM-2B MINI","25.35","62.00 (raw)","13.50 (raw)","4.50 (raw)","27.00 (raw)","19.73 (llm-as-judge)"
"20","Microsoft","Phi-4-mini-instruct","microsoft/phi-4-mini-instruct","22.45","45.90 (raw)","6.56 (raw)","14.40 (raw)","24.87 (raw)","20.53 (llm-as-judge)"
"21","Swiss AI","Apertus-8B-Instruct-2509","swiss-ai/Apertus-8B-Instruct-2509","21.52","56.40 (raw)","4.25 (raw)","6.03 (raw)","20.38 (raw)","20.53 (llm-as-judge)"
"22","ByteDance","Seed-OSS-36B","bytedance/seed-oss-36b-instruct","—","75.67 (raw)","57.00 (raw)","56.05 (raw)","37.66 (raw)","— (llm-as-judge)"
"23","Google","AT&T FT Gemma-3-4B-IT","AT&T/gemma-3-4b-fine-tuned","—","— (raw)","80.09 (raw)","— (raw)","— (raw)","— (llm-as-judge)"